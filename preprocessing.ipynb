{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData():\n",
    "    if not os.path.exists('dataset.csv'):\n",
    "        dfs = []\n",
    "        files = os.listdir('datasets')\n",
    "        for file in files:\n",
    "            if file.endswith('.csv'):\n",
    "                print(file)\n",
    "                df = pd.read_csv(os.path.join('datasets', file))\n",
    "                dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        df.to_csv('dataset.csv', index=False)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tarik\\AppData\\Local\\Temp\\ipykernel_20208\\1811304781.py:16: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv('dataset.csv')\n"
     ]
    }
   ],
   "source": [
    "data = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>body</th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Young Esposito &lt;Young@iworld.de&gt;</td>\n",
       "      <td>user4@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 16:31:02 -0700</td>\n",
       "      <td>Never agree to be a loser</td>\n",
       "      <td>Buck up, your troubles caused by small dimensi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mok &lt;ipline's1983@icable.ph&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 18:31:03 -0500</td>\n",
       "      <td>Befriend Jenna Jameson</td>\n",
       "      <td>\\nUpgrade your sex and pleasures with these te...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daily Top 10 &lt;Karmandeep-opengevl@universalnet...</td>\n",
       "      <td>user2.9@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 20:28:00 -1200</td>\n",
       "      <td>CNN.com Daily Top 10</td>\n",
       "      <td>&gt;+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Michael Parker &lt;ivqrnai@pobox.com&gt;</td>\n",
       "      <td>SpamAssassin Dev &lt;xrh@spamassassin.apache.org&gt;</td>\n",
       "      <td>Tue, 05 Aug 2008 17:31:20 -0600</td>\n",
       "      <td>Re: svn commit: r619753 - in /spamassassin/tru...</td>\n",
       "      <td>Would anyone object to removing .so from this ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gretchen Suggs &lt;externalsep1@loanofficertool.com&gt;</td>\n",
       "      <td>user2.2@gvc.ceas-challenge.cc</td>\n",
       "      <td>Tue, 05 Aug 2008 19:31:21 -0400</td>\n",
       "      <td>SpecialPricesPharmMoreinfo</td>\n",
       "      <td>\\nWelcomeFastShippingCustomerSupport\\nhttp://7...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sender  \\\n",
       "0                   Young Esposito <Young@iworld.de>   \n",
       "1                       Mok <ipline's1983@icable.ph>   \n",
       "2  Daily Top 10 <Karmandeep-opengevl@universalnet...   \n",
       "3                 Michael Parker <ivqrnai@pobox.com>   \n",
       "4  Gretchen Suggs <externalsep1@loanofficertool.com>   \n",
       "\n",
       "                                         receiver  \\\n",
       "0                     user4@gvc.ceas-challenge.cc   \n",
       "1                   user2.2@gvc.ceas-challenge.cc   \n",
       "2                   user2.9@gvc.ceas-challenge.cc   \n",
       "3  SpamAssassin Dev <xrh@spamassassin.apache.org>   \n",
       "4                   user2.2@gvc.ceas-challenge.cc   \n",
       "\n",
       "                              date  \\\n",
       "0  Tue, 05 Aug 2008 16:31:02 -0700   \n",
       "1  Tue, 05 Aug 2008 18:31:03 -0500   \n",
       "2  Tue, 05 Aug 2008 20:28:00 -1200   \n",
       "3  Tue, 05 Aug 2008 17:31:20 -0600   \n",
       "4  Tue, 05 Aug 2008 19:31:21 -0400   \n",
       "\n",
       "                                             subject  \\\n",
       "0                          Never agree to be a loser   \n",
       "1                             Befriend Jenna Jameson   \n",
       "2                               CNN.com Daily Top 10   \n",
       "3  Re: svn commit: r619753 - in /spamassassin/tru...   \n",
       "4                         SpecialPricesPharmMoreinfo   \n",
       "\n",
       "                                                body  label urls  \n",
       "0  Buck up, your troubles caused by small dimensi...    1.0    1  \n",
       "1  \\nUpgrade your sex and pleasures with these te...    1.0    1  \n",
       "2  >+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+=+...    1.0    1  \n",
       "3  Would anyone object to removing .so from this ...    0.0    1  \n",
       "4  \\nWelcomeFastShippingCustomerSupport\\nhttp://7...    1.0    1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'urls': 'url_count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 185474 entries, 0 to 185473\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   sender     184519 non-null  object \n",
      " 1   receiver   178821 non-null  object \n",
      " 2   date       182172 non-null  object \n",
      " 3   subject    182495 non-null  object \n",
      " 4   body       184840 non-null  object \n",
      " 5   label      184688 non-null  float64\n",
      " 6   url_count  184688 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>184688.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.485619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.499794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               label\n",
       "count  184688.000000\n",
       "mean        0.485619\n",
       "std         0.499794\n",
       "min         0.000000\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         1.000000\n",
       "max         1.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tarik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tarik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tarik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['label'], inplace=True)\n",
    "data.dropna(subset=['body'], inplace=True)\n",
    "data['date'].fillna(data['date'].mode()[0], inplace=True)\n",
    "data['url_count'] = pd.to_numeric(data['url_count'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "text_columns = ['sender', 'receiver', 'subject']\n",
    "data[text_columns] = data[text_columns].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['email_type'] = data['subject'].apply(lambda x: 1 if re.search('^re:', x, re.IGNORECASE) else 2 if re.search('^fwd:', x, re.IGNORECASE) else 0)\n",
    "\n",
    "data['subject'] = data['subject'].apply(lambda x: re.sub('^re:', '', x, flags=re.IGNORECASE))\n",
    "data['subject'] = data['subject'].apply(lambda x: re.sub('^fwd:', '', x, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Lowercasing and cleaning\n",
    "data['sender'] = data['sender'].apply(lambda x: x.lower())\n",
    "data['receiver'] = data['receiver'].apply(lambda x: x.lower())\n",
    "data['subject'] = data['subject'].apply(clean_text)\n",
    "data['body'] = data['body'].apply(clean_text)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stop_words = ['subject', 're', 'edu', 'use', 'http', 'https', 'www', 'html', 'index', 'com', 'net', 'org', 'ect', 'hou', 'cc', 'recipient', 'na', 'pm', 'am', 'et', 'enron']\n",
    "stop_words = stop_words.union(additional_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 3 ]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "data['body'] = data['body'].apply(tokenize_and_lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "data['body'] = data['body'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_number_count(text):\n",
    "    return len(re.findall(r'\\d+', text))\n",
    "\n",
    "data['number_count'] = data['body'].apply(find_number_count)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "data['body'] = data['body'].apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'], errors='coerce', utc=True)\n",
    "if data['date'].dtype == 'datetime64[ns, UTC]':\n",
    "    data['day_of_week'] = data['date'].dt.day_name()\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['time_of_day'] = data['date'].dt.hour\n",
    "    data['is_weekend'] = data['date'].dt.dayofweek.apply(lambda x: 1 if x >= 5 else 0)\n",
    "    data['part_of_day'] = data['date'].dt.hour.apply(lambda x: 'morning' if 5 <= x <= 11 else 'afternoon' if 12 <= x <= 17 else 'evening' if 18 <= x <= 22 else 'night')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mail(text):\n",
    "    mail = re.findall(r'\\<(.*?)\\>', text)\n",
    "    if len(mail) > 0:\n",
    "        return mail[0]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "data['sender_mail'] = data['sender'].apply(get_mail)\n",
    "data['sender'] = data['sender'].apply(lambda x: x.split('<')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of Text\n",
    "data['body_length'] = data['body'].apply(len)\n",
    "data['subject_length'] = data['subject'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "data['body_word_count'] = data['body'].apply(lambda x: len(x.split()))\n",
    "data['subject_word_count'] = data['subject'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Count\n",
    "data['body_char_count'] = data['body'].apply(lambda x: sum(len(word) for word in x.split()))\n",
    "data['subject_char_count'] = data['subject'].apply(lambda x: sum(len(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "data['body_sentiment'] = data['body'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "data['subject_sentiment'] = data['subject'].apply(lambda x: TextBlob(x).sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "suspicious_words = [\n",
    "    # Avoid spam words that make exaggerated claims and promises\n",
    "    \"100% more\", \"100% free\", \"100% satisfied\", \"Additional income\", \"Be your own boss\",\n",
    "    \"Best price\", \"Big bucks\", \"Billion\", \"Cash bonus\", \"Cents on the dollar\", \"Consolidate debt\",\n",
    "    \"Double your cash\", \"Double your income\", \"Earn extra cash\", \"Earn money\", \"Eliminate bad credit\",\n",
    "    \"Extra cash\", \"Extra income\", \"Expect to earn\", \"Fast cash\", \"Financial freedom\", \"Free access\",\n",
    "    \"Free consultation\", \"Free gift\", \"Free hosting\", \"Free info\", \"Free investment\", \"Free membership\",\n",
    "    \"Free money\", \"Free preview\", \"Free quote\", \"Free trial\", \"Full refund\", \"Get out of debt\",\n",
    "    \"Get paid\", \"Giveaway\", \"Guaranteed\", \"Increase sales\", \"Increase traffic\", \"Incredible deal\",\n",
    "    \"Lower rates\", \"Lowest price\", \"Make money\", \"Million dollars\", \"Miracle\", \"Money back\",\n",
    "    \"Once in a lifetime\", \"One time\", \"Pennies a day\", \"Potential earnings\", \"Prize\", \"Promise\",\n",
    "    \"Pure profit\", \"Risk-free\", \"Satisfaction guaranteed\", \"Save big money\", \"Save up to\", \"Special promotion\",\n",
    "\n",
    "    # Avoid spam words that create unnecessary urgency and pressure\n",
    "    \"Act now\", \"Apply now\", \"Become a member\", \"Call now\", \"Click below\", \"Click here\",\n",
    "    \"Get it now\", \"Do it today\", \"Don’t delete\", \"Exclusive deal\", \"Get started now\",\n",
    "    \"Important information regarding\", \"Information you requested\", \"Instant\", \"Limited time\",\n",
    "    \"New customers only\", \"Order now\", \"Please read\", \"See for yourself\", \"Sign up free\",\n",
    "    \"Take action\", \"This won’t last\", \"Urgent\", \"What are you waiting for?\", \"While supplies last\",\n",
    "    \"Will not believe your eyes\", \"Winner\", \"Winning\", \"You are a winner\", \"You have been selected\",\n",
    "\n",
    "    # Avoid spam words that look like shady, spammy, or unethical behavior\n",
    "    \"Bulk email\", \"Buy direct\", \"Cancel at any time\", \"Check or money order\", \"Congratulations\",\n",
    "    \"Confidentiality\", \"Cures\", \"Dear friend\", \"Direct email\", \"Direct marketing\", \"Hidden charges\",\n",
    "    \"Human growth hormone\", \"Internet marketing\", \"Lose weight\", \"Mass email\", \"Meet singles\",\n",
    "    \"Multi-level marketing\", \"No catch\", \"No cost\", \"No credit check\", \"No fees\", \"No gimmick\",\n",
    "    \"No hidden costs\", \"No hidden fees\", \"No interest\", \"No investment\", \"No obligation\",\n",
    "    \"No purchase necessary\", \"No questions asked\", \"No strings attached\", \"Not junk\", \"Notspam\",\n",
    "    \"Obligation\", \"Passwords\", \"Requires initial investment\", \"Social security number\", \"This isn’t a scam\",\n",
    "    \"This isn’t junk\", \"This isn’t spam\", \"Undisclosed\", \"Unsecured credit\", \"Unsecured debt\",\n",
    "    \"Unsolicited\", \"Valium\", \"Viagra\", \"Vicodin\", \"We hate spam\", \"Weight loss\", \"Xanax\",\n",
    "\n",
    "    # Avoid spam words that are jargon or legalese (and everything else)\n",
    "    \"Accept credit cards\", \"Ad\", \"All new\", \"As seen on\", \"Bargain\", \"Beneficiary\", \"Billing\",\n",
    "    \"Bonus\", \"Cards accepted\", \"Cash\", \"Certified\", \"Cheap\", \"Claims\", \"Clearance\", \"Compare rates\",\n",
    "    \"Credit card offers\", \"Deal\", \"Debt\", \"Discount\", \"Fantastic\", \"In accordance with laws\", \"Income\",\n",
    "    \"Investment\", \"Join millions\", \"Lifetime\", \"Loans\", \"Luxury\", \"Marketing solution\", \"Message contains\",\n",
    "    \"Mortgage rates\", \"Name brand\", \"Offer\", \"Online marketing\", \"Opt in\", \"Pre-approved\", \"Quote\",\n",
    "    \"Rates\", \"Refinance\", \"Removal\", \"Reserves the right\", \"Score\", \"Search engine\", \"Sent in compliance\",\n",
    "    \"Subject to…\", \"Terms and conditions\", \"Trial\", \"Unlimited\", \"Warranty\", \"Web traffic\", \"Work from home\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_suspicious_words(text):\n",
    "    return sum(text.count(word) for word in suspicious_words)\n",
    "\n",
    "data['suspicious_word_count'] = data['body'].apply(count_suspicious_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('preprocessed_data2.pkl', 'wb') as file:\n",
    "    pickle.dump(data, file)\n",
    "\n",
    "print(\"Data saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "preprocessing_pipeline = {\n",
    "    'clean_text': clean_text,\n",
    "    'tokenize_and_lemmatize': tokenize_and_lemmatize,\n",
    "    'remove_html_tags': remove_html_tags,\n",
    "    'get_mail': get_mail,\n",
    "    'count_suspicious_words': count_suspicious_words,\n",
    "    'suspicious_words': suspicious_words,\n",
    "    'remove_numbers': remove_numbers,\n",
    "}\n",
    "\n",
    "with open('preprocessing_pipeline.pkl', 'wb') as file:\n",
    "    pickle.dump(preprocessing_pipeline, file)\n",
    "\n",
    "print(\"Preprocessing pipeline saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
